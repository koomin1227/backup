{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. 선형 회귀(Linear Regression)\n\n* 본 텀프로젝트의 목적은 **경사 하강법(Gradient Descent)를 이용하여 선형 회귀 문제를 해결**하는 것\n\n* 사이킷런 [Linear Models](https://scikit-learn.org/stable/modules/linear_model.html)을 참조하면 선형 회귀 모델에 대한 설명을 아래 사진과 같이 확인 가능\n\n<center>\n<img src=\"https://drive.google.com/uc?id=1KboYt4oBL1sZk3glRgQQPh9YjIBgLOso\" width=\"900\">\n</center>\n\n\n* 즉, 선형 회귀는 학습 데이터셋 $\\{( x_i,y_i)|1\\le i \\le n\\}$ (단, $ x_i=(x_{i1},\\cdots,x_{ik})^{\\rm T}$)에 대한 학습모델을 다음과 같이 각 특성값에 대해 선형이 되도록 가정하는 판별모델\n\n$$\\hat y = \\beta_0 +\\beta_1  x_1+\\cdots +\\beta_k  x_k$$\n\n * 편향(또는 절편) $\\beta_0$와 가중치 $\\beta_i\\, (1\\le i\\le k)$를 합쳐서 모델 파라미터라고 정의\n\n * $k$차원 특성벡터 $ x=(x_1,\\cdots,x_k)^{\\rm T}$를 $ x=(1,x_1,\\cdots,x_k)^{\\rm T}$로 쓰고(즉, $x_0=1$), $\\beta = (\\beta_0,\\beta_1,\\cdots,\\beta_k)^{\\rm T}$로 나타내면 위 모델은 $\\hat y = h_{\\beta}(x)=\\beta^{\\rm T} x$로 간단히 표기 가능\n\n * 모든 훈련샘플의 특성 벡터를 $n\\times (k+1)$행렬 $ X=(x_{ij})$로 나타낼 때, \n$$\\hat {y} = (\\hat y_1,\\cdots,\\hat y_n)^{\\rm T} = X \\beta$$\n\n * $k=1$일 때를 **단순선형회귀 모델**, $k>1$일 때를 **다중선형회귀 모델**이라 정의\n\n\n","metadata":{"id":"UGreHU47ZqyC"}},{"cell_type":"code","source":"# 선형 회귀 문제를 풀기 위해\n# 랜덤하게 데이터 샘플 생성 \n\nimport numpy as np \nimport matplotlib.pyplot as plt \n%matplotlib inline\n\nnp.random.seed(42)\n\nX = 2 * np.random.rand(100,1)\ny = 4 + 3 * X + np.random.randn(100,1)\n\nplt.plot(X,y, 'b.')\nplt.xlabel(\"$x_1$\", fontsize=10)\nplt.ylabel(\"$y$\", rotation=0, fontsize=10)\nplt.axis([0, 2, 0, 15])","metadata":{"id":"pA29RorfH48A","execution":{"iopub.status.busy":"2023-05-12T05:21:13.213842Z","iopub.execute_input":"2023-05-12T05:21:13.214223Z","iopub.status.idle":"2023-05-12T05:21:13.452744Z","shell.execute_reply.started":"2023-05-12T05:21:13.214192Z","shell.execute_reply":"2023-05-12T05:21:13.451695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 각 특성벡터의 첫 번째 좌표에 bias에 대응되는 1을 추가하여 Xb로 수정 \n# 여러 가지 경사 하강법을 직접 구현해 볼 때 사용\n\nXb = np.column_stack((np.ones((100,1)), X)) # bias에 대응 할 수 있도록 1을 추가\n\nX_new = np.array([[0],[2]]) # 위에서 만든 데이터가 아닌 새로운 데이터 샘플 x_new를 정의하여 회귀 계수를 구할 때 마다 테스트에 사용\nXb_new = np.column_stack((np.ones((2,1)), X_new)) # bias에 대응 할 수 있도록 1을 추가","metadata":{"id":"RY5UXqZYpnJI","execution":{"iopub.status.busy":"2023-05-12T05:21:13.454841Z","iopub.execute_input":"2023-05-12T05:21:13.455185Z","iopub.status.idle":"2023-05-12T05:21:13.460973Z","shell.execute_reply.started":"2023-05-12T05:21:13.455131Z","shell.execute_reply":"2023-05-12T05:21:13.459973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sklearn의 LinearRegression을 활용하여 회귀 계수 구해보기\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\nbeta = np.array([lin_reg.intercept_, lin_reg.coef_],dtype=float) # 회귀 계수 받아오기\n\ny_predict = lin_reg.predict(X_new) # X_new에 대해서 y값을 선형 회귀 모델을 이용하여 예측","metadata":{"id":"qfZqxKNDX6iK","execution":{"iopub.status.busy":"2023-05-12T05:21:13.462756Z","iopub.execute_input":"2023-05-12T05:21:13.463638Z","iopub.status.idle":"2023-05-12T05:21:13.477824Z","shell.execute_reply.started":"2023-05-12T05:21:13.463595Z","shell.execute_reply":"2023-05-12T05:21:13.476720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 모델의 예측을 그래프에 나타내기 \nplt.plot(X, y, 'b.')\nplt.plot(X_new, y_predict, 'r-')\nplt.xlabel(\"$x_1$\", fontsize=10)\nplt.ylabel(\"$y$\", rotation=0, fontsize=10)\nplt.axis([0, 2, 0, 15])\nplt.title(\"Linear regression with Scikit Learn\")","metadata":{"id":"t36341pYppDx","execution":{"iopub.status.busy":"2023-05-12T05:21:13.481191Z","iopub.execute_input":"2023-05-12T05:21:13.481648Z","iopub.status.idle":"2023-05-12T05:21:13.755454Z","shell.execute_reply.started":"2023-05-12T05:21:13.481605Z","shell.execute_reply":"2023-05-12T05:21:13.754406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1-1. 회귀 계수 결정법 (Direct Solution)\n\n\n* 선형회귀에서 주로 사용하는 Mean Squared Error(MSE) 손실 함수를 사용하면 아래로 볼록한(Convex) 형태의 손실함수를 가짐\n\n$$\n\\begin{aligned}\n&\\\\\nL(\\beta) & = \\left\\|\\hat{y}-y \\right\\|_{2}^{2}\\\\\n& = \\left\\|\\beta X-y \\right\\|_{2}^{2}\\\\\n& = (\\beta X-y)^{\\rm T}(\\beta X-y)\\\\\n& = \\beta^{\\rm T}X^{\\rm T}X\\beta-\\beta^{\\rm T}X^{\\rm T}y-y^{\\rm T}X\\beta+y^{\\rm T}y \\\\\n\\end{aligned}$$\n\n\\\n\n* 아래로 볼록한(Convex) 형태의 손실함수의 최소값은 미분이 0이 되는 지점($\\nabla_{\\beta} L(\\beta)=0$)을 통해 명시적인 해를 구할 수 있음\n\n\n$$\n\\begin{aligned}\n&\\\\\n\\nabla_{\\beta} L(\\beta)& =X^{\\rm T}X\\beta+X^{\\rm T}X\\beta-2X^{\\rm T}y=0\\\\\n& \\Rightarrow  2X^{\\rm T}X\\beta=2X^{\\rm T}y\\\\\n& \\Rightarrow  X^{\\rm T}X\\beta=X^{\\rm T}y\\\\\n\\end{aligned}$$\n\n\n\n$$\\therefore \\beta=(X^{\\rm T}X)^{-1}X^{\\rm T}y$$\n\n\n","metadata":{"id":"U00KpNaIaEqS"}},{"cell_type":"code","source":"####### Empty Module.1 #######\n# Direct Solution을 통해서 최적의 Beta를 찾아보세요\n# 위의 수식을 참고하여 최적의 beta를 찾아보세요\na = np.matmul(Xb.transpose(), Xb)   #Xb transpose 와 Xb값을 곱함, 각각의 행의 값 2개를 제곱해서 더한것과 같음\nb = np.matmul(Xb.transpose(), y)    #Xb transpose 와 y값을 곱함, 각각의 Xb행의 값과 Y값 곱한것을 더함\nbeta_direct = np.linalg.inv(a) @ b  #회귀계수를 구함(주어진 식을 이용함)\n\n# 찾은 최적의 beta를 가지고 X_new에 대한 y값을 예측 해보세요\ny_predict_direct = beta_direct[0] + beta_direct[1] * X_new  #구한 회귀변수 B0와 B1을 정의역(0, 2)에서 연결되도록 y예측\n##############################","metadata":{"id":"1UlHd3dNQ954","execution":{"iopub.status.busy":"2023-05-12T05:21:13.756681Z","iopub.execute_input":"2023-05-12T05:21:13.756990Z","iopub.status.idle":"2023-05-12T05:21:13.763439Z","shell.execute_reply.started":"2023-05-12T05:21:13.756961Z","shell.execute_reply":"2023-05-12T05:21:13.762419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 모델의 예측을 그래프에 나타내기 \nplt.plot(X, y, 'b.')\nplt.plot(X_new, y_predict_direct, 'r-')\nplt.xlabel(\"$x_1$\", fontsize=10)\nplt.ylabel(\"$y$\", rotation=0, fontsize=10)\nplt.axis([0, 2, 0, 15])\nplt.title(\"Linear regression with Direct Search(Least Square)\")","metadata":{"id":"M3p92WmM5mXG","execution":{"iopub.status.busy":"2023-05-12T05:21:13.764645Z","iopub.execute_input":"2023-05-12T05:21:13.764955Z","iopub.status.idle":"2023-05-12T05:21:14.047502Z","shell.execute_reply.started":"2023-05-12T05:21:13.764926Z","shell.execute_reply":"2023-05-12T05:21:14.046409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1-2. 회귀 계수 결정법 (Numerical Search)\n\n* 경사하강법(gradient descent)와 같은 반복적인 방식으로 선형회귀 계수를 구할 수 있음\n\n* 경사(gradient)란? **\"임의의 지점에서 함수의 최대 증가 방향\"**을 의미\n$$ \\text{경사(gradient)} ⇒ \\nabla_{\\beta}L(\\beta)=[\\frac{\\partial L(\\beta)}{\\partial \\beta_{0}},\\cdots ,\\frac{\\partial L(\\beta)}{\\partial \\beta_{k}}]\\in R^{k+1}$$\n\n\n* 결국 경사하강법은 최대 증가의 반대 방향인 최대 감소의 방향으로 회귀 계수를 업데이트 하겠다는 것\n$$ \\text{경사하강법(gradient descent)} ⇒ \\beta^{(t+1)}=\\beta^{(t)}-\\eta \\nabla_{\\beta}L(\\beta)$$\n\n\n<center>\n<img src=\"https://drive.google.com/uc?id=1PulS6p9zKdUsV9g0KPPP68ThA39gJirG\" width=\"480\">\n</center>\n","metadata":{"id":"usdrF3_xpkER"}},{"cell_type":"markdown","source":"#### 배치 경사 하강법(Batch Gradient Descent) : 구현 \n\n* 파라미터를 업데이트 할 때마다 모든 학습 데이터를 사용하여 cost function의 gradient를 계산\n\n* Vanilla Gradient Descent라 불림\n\n* 모든 학습 데이터를 사용하기 때문에 gradient의 방향성은 정확하지만 연산이 오래걸려 학습 효율이 좋지 못함\n\n$$\\nabla_{\\beta} L(\\beta) = \\dfrac 2 n  X^{\\rm T}( X\\beta- y) \\; : \\; \\text{gradient}$$\n\n$$\\beta^{(t+1)}⇐\\beta^{(t)}-\\eta \\nabla_{\\beta}L(\\beta) \\; : \\; \\text{update rule}$$","metadata":{"id":"TV7sSWzcqAio"}},{"cell_type":"code","source":"beta_bgd_path = []\neta = 0.1 # 학습률 \nn_epochs = 1000 # epoch 수 \nn = 100 # 샘플수 \n\nbeta_bgd = np.random.randn(2,1)  # 무작위로 beta 초깃값 설정 \n\n####### Empty Module.2 #######\n# Batch Gradient Descent를 통해서 최적의 Beta를 찾아보세요\nfor iteration in range(n_epochs):\n    # 배치 경사 하강법에 해당하는 gradient를 계산하세요\n    gradients = (2 / n) * np.matmul(Xb.transpose(), (Xb @ beta_bgd - y))  #gradient식을 이용해서 행렬곱을 적절히 수행해 gradients를 구함\n    # update rule에 따라서 beta_bgd를 update 하세요\n    beta_bgd = beta_bgd - eta * gradients                                 #update rule식을 이용해서 beta_bgd의 값을 학습률만큼 업데이트 함\n    \n    beta_bgd_path.append(beta_bgd)\n    \n# 얻은 회귀 계수를 가지고 X_new에 대해서 y값을 예측하세요\ny_predict_bgd = beta_bgd[0] + beta_bgd[1] * X_new                         #구한 회귀변수 B0와 B1을 정의역(0, 2)에서 연결되도록 y예측\n##############################","metadata":{"id":"_V7Ec6Trp9bQ","execution":{"iopub.status.busy":"2023-05-12T05:21:14.048967Z","iopub.execute_input":"2023-05-12T05:21:14.049304Z","iopub.status.idle":"2023-05-12T05:21:14.064973Z","shell.execute_reply.started":"2023-05-12T05:21:14.049273Z","shell.execute_reply":"2023-05-12T05:21:14.063621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 모델의 예측을 그래프에 나타내기 \nplt.plot(X, y, 'b.')\nplt.plot(X_new, y_predict_bgd, 'r-')\nplt.xlabel(\"$x_1$\", fontsize=10)\nplt.ylabel(\"$y$\", rotation=0, fontsize=10)\nplt.axis([0, 2, 0, 15])\nplt.title(f\"Linear regression with BGD\")","metadata":{"id":"QyIWGDuG5rsx","execution":{"iopub.status.busy":"2023-05-12T05:21:14.066385Z","iopub.execute_input":"2023-05-12T05:21:14.066825Z","iopub.status.idle":"2023-05-12T05:21:14.347464Z","shell.execute_reply.started":"2023-05-12T05:21:14.066781Z","shell.execute_reply":"2023-05-12T05:21:14.346598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 확률적 경사 하강법(Stochastic Gradient Descent) : 구현 \n\n* 파라미터를 업데이트 할 때, 무작위로 샘플링된 학습 데이터를 하나씩만 이용하여 cost function의 gradient를 계산\n\n* 모델을 자주 업데이트 하며, 성능 개선 정도를 빠르게 확인 가능\n\n* Local minima에 빠질 가능성을 줄일 수 있음\n\n$$\\nabla_{\\beta} L(\\beta) = 2 x_{i}^{\\rm T}(x_{i}^{\\rm T}\\beta- y) \\; : \\; \\text{gradient}$$\n\n$$\\beta^{(t+1)}⇐\\beta^{(t)}-\\eta \\nabla_{\\beta}L(\\beta) \\; : \\; \\text{update rule}$$","metadata":{"id":"T-pcHhsIqNeB"}},{"cell_type":"code","source":"beta_sgd_path = []\nn_epochs = 50\nt0, t1 = 5, 50  # 학습 스케줄 하이퍼파라미터 \nn = 100         # 샘플 수 \n\ndef learning_schedule(t):\n    return t0/(t+t1)\n\nbeta_sgd = np.random.randn(2,1)  # beta 무작위 초기화 \n\n####### Empty Module.3 #########\n# Stochastic Gradient Descent를 통해서 최적의 Beta를 찾아보세요\nfor epoch in range(n_epochs):\n    for i in range(n):\n        eta = learning_schedule(epoch * n + i)\n\n        # 0 ~ n-1 까지 랜덤하게 인덱스 선택\n        random_idx = np.random.randint(n)\n        # random_idx를 활용해 샘플 하나 선택\n        tx = Xb[random_idx]                                     #기존 Xb값에서 랜덤하게 tx값 선택\n        tx = tx.reshape(2,1)                                    #transpose를 하기 위해 tx값을 reshape\n        # random_idx를 활용해 샘플 하나 선택\n        ty = y[random_idx]                                      #기존 y값에서 랜덤하게 ty값 선택\n\n        # 확률적 경사 하강법에 해당하는 gradient를 계산하세요\n        gradients = 2 * tx * (tx.transpose() @ beta_sgd - ty)   #gradient식을 이용해서 행렬곱을 적절히 수행해 gradients를 구함\n        # update rule에 따라서 beta_sgd를 update 하세요\n        beta_sgd = beta_sgd - eta * gradients                   #update rule식을 이용해서 beta_sgd의 값을 학습률만큼 업데이트 함\n        beta_sgd_path.append(beta_sgd)\n        \n# 얻은 회귀 계수를 가지고 X_new에 대해서 y값을 예측하세요\ny_predict_sgd = beta_sgd[0] + beta_sgd[1] * X_new               #구한 회귀변수 B0와 B1을 정의역(0, 2)에서 연결되도록 y예측\n##############################","metadata":{"id":"h679VfimqK5P","execution":{"iopub.status.busy":"2023-05-12T05:21:14.348715Z","iopub.execute_input":"2023-05-12T05:21:14.349205Z","iopub.status.idle":"2023-05-12T05:21:14.433211Z","shell.execute_reply.started":"2023-05-12T05:21:14.349174Z","shell.execute_reply":"2023-05-12T05:21:14.432147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 모델의 예측을 그래프에 나타내기 \nplt.plot(X, y, 'b.')\nplt.plot(X_new, y_predict_sgd, 'r-')\nplt.xlabel(\"$x_1$\", fontsize=10)\nplt.ylabel(\"$y$\", rotation=0, fontsize=10)\nplt.axis([0, 2, 0, 15])\nplt.title(f\"Linear regression with SGD\")","metadata":{"id":"xzywjap25v-N","execution":{"iopub.status.busy":"2023-05-12T05:21:14.436829Z","iopub.execute_input":"2023-05-12T05:21:14.437164Z","iopub.status.idle":"2023-05-12T05:21:14.715957Z","shell.execute_reply.started":"2023-05-12T05:21:14.437132Z","shell.execute_reply":"2023-05-12T05:21:14.714809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 미니 배치 경사 하강법(Mini Batch Gradient Descent) : 구현 \n\n* 파라미터를 업데이트 할 때마다 일정량의 일부 데이터를 무작위로 뽑아 cost function의 gradient를 계산\n\n* Batch Gradient Descent와 Stochastic Gradient Descent 개념의 혼합\n\n* SGD의 노이즈를 줄이면서, GD의 전체 배치보다 효율적\n\n$$\\nabla_{\\beta}L(\\beta)=\\frac{2}{m}\\sum_{i=1}^{m} x_{i}^{\\rm T}(x_{i}^{\\rm T}\\beta-y) \\; : \\; \\text{gradient}$$\n\n$$\\beta^{(t+1)}⇐\\beta^{(t)}-\\eta \\nabla_{\\beta}L(\\beta) \\; : \\; \\text{update rule}$$","metadata":{"id":"FXZmc6MgvgCY"}},{"cell_type":"code","source":"beta_mgd_path = []\n\nn_epochs = 50\nminibatch_size = 20\n\nnp.random.seed(42)\nbeta_mgd = np.random.randn(2,1)  # 랜덤 초기화\n\nt0, t1 = 200, 1000\ndef learning_schedule(t):\n    return t0 / (t + t1)\n\nt = 0\n\n####### Empty Module.4 #########\n# Mini Batch Gradient Descent를 통해서 최적의 Beta를 찾아보세요.\nfor epoch in range(n_epochs):\n    shuffled_indices = np.random.permutation(n)\n    Xb_shuffled = Xb[shuffled_indices]\n    y_shuffled = y[shuffled_indices]\n    for i in range(0, n, minibatch_size):\n        t += 1\n        eta = learning_schedule(t)\n        # minibatch_size를 활용해 batch 단위로 샘플링\n        tx = Xb_shuffled[i:i+minibatch_size]                                  #섞은 Xb값에서 minibatch_size만큼의 tx를 샘플링\n        # minibatch_size를 활용해 batch 단위로 샘플링\n        ty = y_shuffled[i:i+minibatch_size]                                   #섞은 y값에서 minibatch_size만큼의 ty를 샘플링\n        # 확률적 경사 하강법에 해당하는 gradient를 계산하세요 \n        gradients = 2/minibatch_size * tx.transpose() @ (tx @ beta_mgd - ty)  #gradient식을 이용해서 행렬곱을 적절히 수행해 gradients를 구함\n        # update rule에 따라서 beta_mgd를 update 하세요\n        beta_mgd = beta_mgd - eta * gradients                                 #update rule식을 이용해서 beta_bgd의 값을 학습률만큼 업데이트 함\n        beta_mgd_path.append(beta_mgd)\n        \n# 얻은 회귀 계수를 가지고 X_new에 대해서 y값을 예측하세요\ny_predict_mgd = beta_mgd[0] + beta_mgd[1] * X_new                             #구한 회귀변수 B0와 B1을 정의역(0, 2)에서 연결되도록 y예측\n##############################","metadata":{"id":"tmoK2KeuqVfE","execution":{"iopub.status.busy":"2023-05-12T05:21:14.717331Z","iopub.execute_input":"2023-05-12T05:21:14.717679Z","iopub.status.idle":"2023-05-12T05:21:14.731393Z","shell.execute_reply.started":"2023-05-12T05:21:14.717647Z","shell.execute_reply":"2023-05-12T05:21:14.730104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 모델의 예측을 그래프에 나타내기 \nplt.plot(X, y, 'b.')\nplt.plot(X_new, y_predict_mgd, 'r-')\nplt.xlabel(\"$x_1$\", fontsize=10)\nplt.ylabel(\"$y$\", rotation=0, fontsize=10)\nplt.axis([0, 2, 0, 15])\nplt.title(f\"Linear regression with MGD\")","metadata":{"id":"FbSfX2M551vG","execution":{"iopub.status.busy":"2023-05-12T05:21:14.732858Z","iopub.execute_input":"2023-05-12T05:21:14.733304Z","iopub.status.idle":"2023-05-12T05:21:15.007769Z","shell.execute_reply.started":"2023-05-12T05:21:14.733265Z","shell.execute_reply":"2023-05-12T05:21:15.006594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 파라미터 공간에 표시된 경사 하강법의 경로 비교 \n\n* 앞에서 계산해둔 `beta_bgd_path`, `beta_sgd_path`, `beta_mgd_path`에 각각 순차적으로 저장된 $(\\beta_0,\\beta_1)$값을 이용","metadata":{"id":"HcMc-bqVqXxq"}},{"cell_type":"code","source":"beta_bgd_path = np.array(beta_bgd_path)\nbeta_sgd_path = np.array(beta_sgd_path)\nbeta_mgd_path = np.array(beta_mgd_path)\n\nplt.figure(figsize=(7,4))\nplt.plot(beta_sgd_path[:, 0], beta_sgd_path[:, 1], \"r-x\", linewidth=1.5, label=\"Stochastic\")\nplt.plot(beta_mgd_path[:, 0], beta_mgd_path[:, 1], \"g-+\", linewidth=1, label=\"Mini-batch\")\nplt.plot(beta_bgd_path[:, 0], beta_bgd_path[:, 1], \"b-o\", linewidth=1.5, label=\"Batch\")\nplt.legend(loc=\"lower right\", fontsize=10)\nplt.xlabel(r\"$\\beta_0$\", fontsize=10)\nplt.ylabel(r\"$\\beta_1$   \", fontsize=10, rotation=0)\nplt.axis([2.5, 4.25, 2.3, 3.9])","metadata":{"id":"06lCzkfpqZFm","execution":{"iopub.status.busy":"2023-05-12T05:21:15.009617Z","iopub.execute_input":"2023-05-12T05:21:15.009947Z","iopub.status.idle":"2023-05-12T05:21:15.324040Z","shell.execute_reply.started":"2023-05-12T05:21:15.009916Z","shell.execute_reply":"2023-05-12T05:21:15.323230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"{:36s} : {}, {}\".format(\"사이킷런 회귀 계수\",beta[0],beta[1]))\nprint(\"{:35s} : {}, {}\".format(\"직접 구현한 회귀 계수(Direct Search)\",beta_direct[0],beta_direct[1]))\nprint(\"{:35s} : {}, {}\".format(\"직접 구현한 회귀 계수(Batch Descent)\",beta_bgd[0],beta_bgd[1]))\nprint(\"{:35s} : {}, {}\".format(\"직접 구현한 회귀 계수(Stochastic Descent)\",beta_sgd[0],beta_sgd[1]))\nprint(\"{:35s} : {}, {}\".format(\"직접 구현한 회귀 계수(Mini-Batch Descent)\",beta_mgd[0],beta_mgd[1]))","metadata":{"id":"wFGXcLkFxOZB","execution":{"iopub.status.busy":"2023-05-12T05:21:15.325332Z","iopub.execute_input":"2023-05-12T05:21:15.325971Z","iopub.status.idle":"2023-05-12T05:21:15.333850Z","shell.execute_reply.started":"2023-05-12T05:21:15.325938Z","shell.execute_reply":"2023-05-12T05:21:15.332811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## 2. 선형 분류(Linear Classification)\n\n* 본 텀프로젝트의 목적은 **경사 하강법(Gradient Descent)를 이용하여 선형 분류 문제를 해결**하는 것\n\n* 사이킷런 [Linear Models](https://scikit-learn.org/stable/modules/linear_model.html)을 참조하면 선형 분류 모델에 대한 설명을 아래 사진과 같이 확인 가능\n\n<center>\n<img src=\"https://drive.google.com/uc?id=1hfO_iU1tDg2pyFiyw2h5AeLi3Ro-p4-X\" width=\"900\">\n</center>\n\n* 레이블이 $1$, $0$인 두 개의 클래스에 대한 분류문제에서 샘플이 특정 클래스에 속할 확률을 추정하는 지도학습의 한 가지 (Binary case)\n\n\n\n* 선형회귀 모델과 같이 입력 특성의 가중치의 합(편향 포함) ${\\beta}^{\\rm T} x = \\beta_0+\\beta_1 x_1+\\cdots +\\beta_nx_n$을 계산한 다음 시그모이드 함수(sigmoid) $\\sigma(t)=\\dfrac{1}{1+\\exp(-t)}$를 취한 값 $\\sigma({\\beta}^{\\rm T} x)$를 ${\\rm P}(Y=1|X=x)$에 대한 추정값 $\\hat p(x)$로 추정하는 모델.   \n\n\n* 즉, 모델 파라미터 ${\\beta}=(\\beta_0,\\cdots,\\beta_n)^{\\rm T}$에 대한 로지스틱 회귀 모델을 $h_{{\\beta}}$라 할 때\n$$ $$\n$\\quad \\quad \n\\hat p(x) = h_{{\\beta}}(x)= \\sigma({\\beta}^{\\rm T}x)\n=\\dfrac{1}{1+\\exp(-{\\beta}^{\\rm T}x)}=\\dfrac{1}{1+\\exp\\bigl(-(\\beta_0+\\beta_1x_1+\\cdots+\\beta_n x_n)\\bigr)} \n$  <span style=\"color:blue\">$\\cdots\\cdots$ (1)</span>\n\n\n* <span style=\"color:blue\"> 로지스틱 회귀 모델을 통한 레이블의 예측 :</span>    \n\n* 샘플 $x$가 양성 클래스(y=1)에 속할 확률 $\\hat p(x)=h_{{\\beta}}(x)$를 추정한 후 다음과 같이 예측 $\\hat y$를 구함 \n$$ $$\n$$\n\\hat y = \\begin{cases} 0 & \\text{ if }\\hat p(x)<0.5\\\\ 1 &\\text{ if }\\hat p(x)\\ge0.5\n\\end{cases}\n$$\n","metadata":{"id":"TtwRoIwqINmA"}},{"cell_type":"code","source":"# 시그모이드 함수 시각화\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\n\nxlist = np.linspace(-10,10,1000)\nylist = sigmoid(xlist)\n\nplt.plot(xlist,ylist, label=r\"$\\sigma (t)=\\dfrac{1}{1+\\exp(-t)}$\")\n\nplt.plot([-10,10],[0.5,0.5], 'r--', linewidth=0.6)\nplt.plot([-10,10],[1,1], 'r--', linewidth=0.6)\nplt.plot([-10,10],[0,0], 'r--', linewidth=0.6)\nplt.plot([0,0],[0,1],'r--', linewidth=0.6)\n\nplt.legend(loc=\"upper left\", fontsize=10)","metadata":{"id":"InBXpUVBcmVm","execution":{"iopub.status.busy":"2023-05-12T05:21:15.335203Z","iopub.execute_input":"2023-05-12T05:21:15.336245Z","iopub.status.idle":"2023-05-12T05:21:15.596897Z","shell.execute_reply.started":"2023-05-12T05:21:15.336210Z","shell.execute_reply":"2023-05-12T05:21:15.595791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**참고) 시그모이드 함수 미분** \n> $$\n\\begin{aligned}\n&\\\\\n\\sigma (x)& =\\frac{1}{1+e^{-x}} \\\\\n\\sigma' (x)& =\\frac{0\\cdot \\left( 1+e^{-x}\\right)  -1\\cdot (e^{-x}\\times -1)}{(1+e^{-x})^{2}}\\\\\n& =  \\frac{e^{-x}}{(1+e^{-x})^{2}} =\\frac{1-1+e^{-x}}{(1+e^{-x})^{2}} =\\frac{1+e^{-x}}{\\left( 1+e^{-x}\\right)^{2}  } -\\frac{1}{\\left( 1+e^{-x}\\right)^{2}  }\\\\\n& =  \\frac{1}{1+e^{-x}} \\left( 1-\\frac{1}{1+e^{-x}} \\right)  =\\sigma (x)\\left( 1-\\sigma (x)\\right)\\\\\n\\end{aligned}$$","metadata":{"id":"22utuT0dzGlx"}},{"cell_type":"markdown","source":"## 2-1. 로지스틱 회귀모델의 손실 함수 \n\n* 훈련 데이터셋 $\\{(x_i,y_i)|1\\le i \\le n\\}$이 주어질 때, 다음과 같이 정의되는 로지스틱 회귀의 비용함수 \n$$ $$\n$L(\\beta) = -\\dfrac 1 n \\sum_{i=1}^n \\left(y_i \\ln p_i + (1-y_i)\\ln (1-p_i)\\right)\\quad $ (단, $p_i= \\sigma(\\beta^{\\rm T}x_i$) <span style=\"color:blue\">\n$$ $$\n가 최소가 되는 모델 파라미터 $\\beta$를 구하는 것   \n\n","metadata":{"id":"KZwugMMmeB0t"}},{"cell_type":"code","source":"from sklearn.datasets import load_iris\niris = load_iris()\n\n# 이진 분류를 위해 iris 데이터에서 두가지 클래스만을 사용\n\nsepal_len = iris['data'][:100,0]\nsepal_wid = iris['data'][:100,1]\nlabels = iris['target'][:100]\n\nsepal_len -= np.mean(sepal_len)\nsepal_wid -= np.mean(sepal_wid)\n\nplt.scatter(sepal_len, \n            sepal_wid,\n            c=labels,\n            cmap=plt.cm.Paired)\nplt.xlabel(\"sepal length\")\nplt.ylabel(\"sepal width\")","metadata":{"id":"CWTRUWTyuseH","execution":{"iopub.status.busy":"2023-05-12T05:21:15.598230Z","iopub.execute_input":"2023-05-12T05:21:15.598588Z","iopub.status.idle":"2023-05-12T05:21:15.813135Z","shell.execute_reply.started":"2023-05-12T05:21:15.598531Z","shell.execute_reply":"2023-05-12T05:21:15.811954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X, y 설정\nX = np.stack([sepal_len, sepal_wid], axis=1)\ny = labels\ny = np.expand_dims(y,axis=1)","metadata":{"id":"BLbK47FexEg-","execution":{"iopub.status.busy":"2023-05-12T05:21:15.814407Z","iopub.execute_input":"2023-05-12T05:21:15.814763Z","iopub.status.idle":"2023-05-12T05:21:15.820714Z","shell.execute_reply.started":"2023-05-12T05:21:15.814719Z","shell.execute_reply":"2023-05-12T05:21:15.819590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_sep(w1, w2, title, color='green'):\n\n    plt.scatter(sepal_len, \n                sepal_wid,\n                c=labels,\n                cmap=plt.cm.Paired)\n    plt.title(title)\n    plt.ylim([-1.5,1.5])\n    plt.xlim([-1.5,2])\n    plt.xlabel(\"sepal length\")\n    plt.ylabel(\"sepal width\")\n    if w2 != 0:\n        m = -w1/w2\n        t = 1 if w2 > 0 else -1\n        plt.plot(\n            [-1.5,2.0], \n            [-1.5*m, 2.0*m], \n            '-y', \n            color=color)\n        plt.fill_between(\n            [-1.5, 2.0],\n            [m*-1.5, m*2.0],\n            [t*1.5, t*1.5],\n            alpha=0.2,\n            color=color)\n    if w2 == 0: # decision boundary is vertical\n        t = 1 if w1 > 0 else -1\n        plt.plot([0, 0],\n                 [-1.5, 2.0],\n                 '-y',\n                color=color)\n        plt.fill_between(\n            [0, 2.0*t],\n            [-1.5, -2.0],\n            [1.5, 2],\n            alpha=0.2,\n            color=color)","metadata":{"id":"9cubdSYOwba_","execution":{"iopub.status.busy":"2023-05-12T05:21:15.822140Z","iopub.execute_input":"2023-05-12T05:21:15.822571Z","iopub.status.idle":"2023-05-12T05:21:15.834955Z","shell.execute_reply.started":"2023-05-12T05:21:15.822514Z","shell.execute_reply":"2023-05-12T05:21:15.833766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sklearn의 LogisticRegression 활용하여 회귀 계수 구해보기\nimport sklearn.linear_model\n\nmodel = sklearn.linear_model.LogisticRegression(fit_intercept=False,random_state=42)\nmodel.fit(X,y)\nplot_sep(model.coef_[0][0], model.coef_[0][1], \"Linear Classification with Logistic Regression\")","metadata":{"id":"pwBZvbewwicM","execution":{"iopub.status.busy":"2023-05-12T05:21:15.836570Z","iopub.execute_input":"2023-05-12T05:21:15.836935Z","iopub.status.idle":"2023-05-12T05:21:16.120354Z","shell.execute_reply.started":"2023-05-12T05:21:15.836899Z","shell.execute_reply":"2023-05-12T05:21:16.119225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2-2. 회귀 계수 결정법 (Numerical Search)\n\n\n* 비용함수 $L(\\beta)$는 $\\beta$에 대해 아래로 볼록한(Convex) 함수이므로 최솟값이 존재함을 보장할 수 있지만, Direct Search 처럼 해를 구하는 공식은 없음  \n\n* 경사하강법 또는 다른 최적화 알고리즘(BFGS, Newton ...)을 이용하여 해의 근삿값을 구함 (`LogisticRegression`의 [solver](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)참고)\n\n\n\n\n\n* 배치 경사하강법을 적용할 때 비용함수에 대한 그래디언트 벡터 $\\nabla_{\\beta}L(\\beta)$ : 각 $i$ ($1\\le i\\le n)$에 대해 $\\nabla_{\\beta}L(\\beta)$의 $j$번째 성분\n\n\n\n$$\n\\begin{aligned}\n&\\\\\n\\dfrac{\\partial L(\\beta)}{\\partial \\beta_j} & = -\\frac{1}{n} \\sum^{n}_{i=1} [y_{i}\\cdot \\left( \\frac{1}{\\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  } \\right)  \\cdot \\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  \\left( 1-\\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  \\right)  \\cdot x^{(n)}_{i}]-[\\left( 1-y_{i}\\right)  \\cdot \\left( \\frac{1}{1-\\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  } \\right)  \\cdot \\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  \\cdot \\left( 1-\\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  \\right)  \\cdot x^{(n)}_{i}]\\\\\n& = -\\frac{1}{n} \\sum^{n}_{i=1} [y_{i}\\cdot \\left( 1-\\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  \\right)  \\cdot x^{(n)}_{i}]+[\\left( 1-y_{i}\\right)  \\cdot \\left( -1\\right)  \\cdot \\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  \\cdot x^{(n)}_{i}] \\\\\n& = -\\frac{1}{n} \\sum^{n}_{i=1} \\left[ y_{i}-y_{i}\\cdot \\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  -\\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  +y_{i}\\cdot \\sigma \\left( \\beta^{T} x_{i}\\right)  \\right]  \\cdot x^{(n)}_{i} \\\\\n& = -\\frac{1}{n} \\sum^{n}_{i=1} \\left[ y_{i}-\\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  \\right]  \\cdot x^{(n)}_{i} \\\\\n& = \\frac{1}{n} \\sum^{n}_{i=1} \\left[ \\sigma \\left( \\beta^{\\rm T} x_{i}\\right)  -y_{i}\\right]  \\cdot x^{(n)}_{i} \\\\\n\\end{aligned}$$\n\n","metadata":{"id":"Nd_VHihY78ts"}},{"cell_type":"markdown","source":"#### 배치 경사 하강법(Batch Gradient Descent) : 구현 \n\n* 파라미터를 업데이트 할 때마다 모든 학습 데이터를 사용하여 cost function의 gradient를 계산\n\n* Vanilla Gradient Descent라 불림\n\n* 모든 학습 데이터를 사용하기 때문에 gradient의 방향성은 정확하지만 연산이 오래걸려 학습 효율이 좋지 못함\n\n$$\\nabla_{\\beta} L(\\beta) = \\dfrac 1 n  X^{\\rm T}( \\sigma (X\\beta)- y) \\; : \\; \\text{gradient}$$\n\n$$\\beta^{(t+1)}⇐\\beta^{(t)}-\\eta \\nabla_{\\beta}L(\\beta) \\; : \\; \\text{update rule}$$","metadata":{"id":"cdtqYQKw1fVo"}},{"cell_type":"code","source":"beta_bgd_path = list()\neta = 0.1 # 학습률 \nn_epochs = 500 # epoch 수 \nn = 100 # 샘플수 \n\nnp.random.seed(42)\nbeta_bgd = np.random.randn(2,1) \n\n####### Empty Module.5 #########\n# Batch Gradient Descent를 통해서 최적의 Beta를 찾아보세요\nfor iteration in range(n_epochs):\n    # 배치 경사 하강법에 해당하는 gradient를 계산하세요 \n    gradients = (1/n)*X.transpose()@(sigmoid(X @ beta_bgd) - y)  #gradient식을 이용해서 행렬곱을 적절히 수행해 gradients를 구함\n    # update rule에 따라서 beta_bgd를 update 하세요\n    beta_bgd = beta_bgd - eta * gradients                        #update rule식을 이용해서 beta_bgd의 값을 학습률만큼 업데이트 함.\n    beta_bgd_path.append(beta_bgd)\n###############################","metadata":{"id":"UiqqBVzfxolQ","execution":{"iopub.status.busy":"2023-05-12T05:21:16.121797Z","iopub.execute_input":"2023-05-12T05:21:16.122336Z","iopub.status.idle":"2023-05-12T05:21:16.139687Z","shell.execute_reply.started":"2023-05-12T05:21:16.122304Z","shell.execute_reply":"2023-05-12T05:21:16.138325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 결정 경계 시각화\nplot_sep(beta_bgd[0][0], beta_bgd[1][0], \"Linear Classification with BGD\")","metadata":{"id":"fKQBnoPf7Gub","execution":{"iopub.status.busy":"2023-05-12T05:21:16.141111Z","iopub.execute_input":"2023-05-12T05:21:16.141432Z","iopub.status.idle":"2023-05-12T05:21:16.419434Z","shell.execute_reply.started":"2023-05-12T05:21:16.141403Z","shell.execute_reply":"2023-05-12T05:21:16.418313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 확률적 경사 하강법(Stochastic Gradient Descent) : 구현 \n\n* 파라미터를 업데이트 할 때, 무작위로 샘플링된 학습 데이터를 하나씩만 이용하여 cost function의 gradient를 계산\n\n* 모델을 자주 업데이트 하며, 성능 개선 정도를 빠르게 확인 가능\n\n* Local minima에 빠질 가능성을 줄일 수 있음\n\n$$\\nabla_{\\beta} L(\\beta) = x^{\\rm T}(\\sigma(x^{\\rm T}\\beta)- y) \\; : \\; \\text{gradient}$$\n\n$$\\beta^{(t+1)}⇐\\beta^{(t)}-\\eta \\nabla_{\\beta}L(\\beta) \\; : \\; \\text{update rule}$$","metadata":{"id":"i64JBT5o1i8F"}},{"cell_type":"code","source":"beta_sgd_path = []\nn_epochs = 500\nt0, t1 = 5, 10  # 학습 스케줄 하이퍼파라미터 \nn = 100         # 샘플 수 \n\ndef learning_schedule(t):\n    return t0/(t+t1)\n    \nnp.random.seed(42)\nbeta_sgd = np.random.randn(2,1)  # beta 무작위 초기화 \n\n####### Empty Module.6 #########\n# Stochastic Gradient Descent를 통해서 최적의 Beta를 찾아보세요\nfor epoch in range(n_epochs):\n    for i in range(n):\n        eta = learning_schedule(epoch * n + i)\n        # 0 ~ n-1 까지 랜덤하게 인덱스 선택\n        random_idx = np.random.randint(n)\n        # random_idx를 활용해 샘플 하나 선택\n        tx = X[random_idx]                                         #기존 X값에서 랜덤하게 tx값 선택\n        tx = tx.reshape(2, 1)                                      #transpose를 하기 위해 reshape\n        # random_idx를 활용해 샘플 하나 선택\n        ty = y[random_idx]                                         #기존 y값에서 랜덤하게 ty값 선택\n        # 확률적 경사 하강법에 해당하는 gradient를 계산하세요\n        gradients = tx @ (sigmoid(tx.transpose() @ beta_sgd) - ty) #gradient식을 이용해서 행렬곱을 적절히 수행해 gradients를 구함\n         # update rule에 따라서 beta_mgd를 update 하세요\n        beta_sgd = beta_sgd - eta * gradients                      #update rule식을 이용해서 beta_bgd의 값을 학습률만큼 업데이트 함.\n\n        beta_sgd_path.append(beta_sgd)\n##############################","metadata":{"id":"myP7pdRu22N9","execution":{"iopub.status.busy":"2023-05-12T05:21:16.421029Z","iopub.execute_input":"2023-05-12T05:21:16.421481Z","iopub.status.idle":"2023-05-12T05:21:17.523710Z","shell.execute_reply.started":"2023-05-12T05:21:16.421438Z","shell.execute_reply":"2023-05-12T05:21:17.522507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 결정 경계 시각화\nplot_sep(beta_sgd[0][0], beta_sgd[1][0], \"Linear Classification with SGD\")","metadata":{"id":"IKDS2WXm7LyC","execution":{"iopub.status.busy":"2023-05-12T05:21:17.525053Z","iopub.execute_input":"2023-05-12T05:21:17.525502Z","iopub.status.idle":"2023-05-12T05:21:17.803758Z","shell.execute_reply.started":"2023-05-12T05:21:17.525458Z","shell.execute_reply":"2023-05-12T05:21:17.802627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 미니 배치 경사 하강법(Mini Batch Gradient Descent) : 구현 \n\n* 파라미터를 업데이트 할 때마다 일정량의 일부 데이터를 무작위로 뽑아 cost function의 gradient를 계산\n\n* Batch Gradient Descent와 Stochastic Gradient Descent 개념의 혼합\n\n* SGD의 노이즈를 줄이면서, GD의 전체 배치보다 효율적\n\n$$\\nabla_{\\beta}L(\\beta)=\\frac{1}{m}\\sum_{i=1}^{m} x_{i}^{\\rm T}(\\sigma (x_{i}^{\\rm T}\\beta)-y) \\; : \\; \\text{gradient}$$\n\n$$\\beta^{(t+1)}⇐\\beta^{(t)}-\\eta \\nabla_{\\beta}L(\\beta) \\; : \\; \\text{update rule}$$","metadata":{"id":"Tk9h0ueu1meF"}},{"cell_type":"code","source":"beta_mgd_path = []\n\nn_epochs = 50\nminibatch_size = 20\n\nnp.random.seed(42)\nbeta_mgd = np.random.randn(2,1)  # 랜덤 초기화\n\nt0, t1 = 200, 1000\ndef learning_schedule(t):\n    return t0 / (t + t1)\n\nt = 0\n\n####### Empty Module.8#########\nfor epoch in range(n_epochs):\n    shuffled_indices = np.random.permutation(n)\n    X_shuffled = X[shuffled_indices]\n    y_shuffled = y[shuffled_indices]\n    for i in range(0, n, minibatch_size):\n        t += 1\n        eta = learning_schedule(t)\n        \n        # minibatch_size를 활용해 batch 단위로 샘플링\n        tx = X_shuffled[i:i+minibatch_size]                                           #섞은 X값에서 minibatch_size만큼의 tx를 샘플링\n        # minibatch_size를 활용해 batch 단위로 샘플링\n        ty = y_shuffled[i:i+minibatch_size]                                           #섞은 y값에서 minibatch_size만큼의 ty를 샘플링\n        # 미니 배치 경사 하강법에 해당하는 gradient를 계산하세요 \n        gradients = 1/minibatch_size * tx.transpose() @ (sigmoid(tx @ beta_mgd) - ty) #gradient식을 이용해서 행렬곱을 적절히 수행해 gradients를 구함\n        # update rule에 따라서 beta_mgd를 update 하세요\n        beta_mgd = beta_mgd - eta*gradients                                           #update rule식을 이용해서 beta_bgd의 값을 학습률만큼 업데이트 함.\n        \n        beta_mgd_path.append(beta_mgd)\n############################### ","metadata":{"id":"EIY-X8xD4AHu","execution":{"iopub.status.busy":"2023-05-12T05:21:17.805099Z","iopub.execute_input":"2023-05-12T05:21:17.805432Z","iopub.status.idle":"2023-05-12T05:21:17.818840Z","shell.execute_reply.started":"2023-05-12T05:21:17.805402Z","shell.execute_reply":"2023-05-12T05:21:17.817829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 결정 경계 시각화\nplot_sep(beta_mgd[0][0], beta_mgd[1][0], \"Linear Classification with MGD\")","metadata":{"id":"Rgy56KAW7PoV","execution":{"iopub.status.busy":"2023-05-12T05:21:17.820284Z","iopub.execute_input":"2023-05-12T05:21:17.820705Z","iopub.status.idle":"2023-05-12T05:21:18.115706Z","shell.execute_reply.started":"2023-05-12T05:21:17.820670Z","shell.execute_reply":"2023-05-12T05:21:18.114620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 파라미터 공간에 표시된 경사 하강법의 경로 비교 \n\n* 앞에서 계산해둔 `beta_bgd_path`, `beta_sgd_path`, `beta_mgd_path`에 각각 순차적으로 저장된 $(\\beta_0,\\beta_1)$값을 이용","metadata":{"id":"6aXdKZwv4eBY"}},{"cell_type":"code","source":"beta_bgd_path = np.array(beta_bgd_path)\nbeta_sgd_path = np.array(beta_sgd_path)\nbeta_mgd_path = np.array(beta_mgd_path)\n\nplt.figure(figsize=(7,4))\nplt.plot(beta_sgd_path[:, 0], beta_sgd_path[:, 1], \"r-x\", linewidth=0.5, label=\"Stochastic\")\nplt.plot(beta_mgd_path[:, 0], beta_mgd_path[:, 1], \"g-+\", linewidth=1, label=\"Mini-batch\")\nplt.plot(beta_bgd_path[:, 0], beta_bgd_path[:, 1], \"b-o\", linewidth=1., label=\"Batch\")\nplt.legend(loc=\"lower right\", fontsize=10)\nplt.xlabel(r\"$\\beta_0$\", fontsize=10)\nplt.ylabel(r\"$\\beta_1$   \", fontsize=10, rotation=0)","metadata":{"id":"_PZjrxUL4daJ","execution":{"iopub.status.busy":"2023-05-12T05:21:18.116956Z","iopub.execute_input":"2023-05-12T05:21:18.117296Z","iopub.status.idle":"2023-05-12T05:21:18.494621Z","shell.execute_reply.started":"2023-05-12T05:21:18.117267Z","shell.execute_reply":"2023-05-12T05:21:18.493628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"{:36s} : {}, {}\".format(\"사이킷런 회귀 계수\",model.coef_[0][0], model.coef_[0][1]))\nprint(\"{:35s} : {}, {}\".format(\"직접 구현한 회귀 계수(Batch Descent)\",beta_bgd[0][0], beta_bgd[1][0]))\nprint(\"{:35s} : {}, {}\".format(\"직접 구현한 회귀 계수(Stochastic Descent)\",beta_sgd[0][0], beta_sgd[1][0]))\nprint(\"{:35s} : {}, {}\".format(\"직접 구현한 회귀 계수(Mini-Batch Descent)\",beta_mgd[0][0], beta_mgd[1][0]))","metadata":{"id":"WODt4oKpxBX-","execution":{"iopub.status.busy":"2023-05-12T05:21:18.495993Z","iopub.execute_input":"2023-05-12T05:21:18.496410Z","iopub.status.idle":"2023-05-12T05:21:18.503938Z","shell.execute_reply.started":"2023-05-12T05:21:18.496370Z","shell.execute_reply":"2023-05-12T05:21:18.502863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 보고서\n\n1. 사이킷런에 구현되어 있는 [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)은 이번 텀프로젝트에서 구현한 경사 하강법(Gradient Descent) 기반의 방식과 어떤 차이가 있는지 자유롭게 서술하세요. [1점]\n\n    * 회귀 계수를 추정하는 과정에서의 다른 부분을 위주로 서술하시면 됩니다.\n\n2. 사이킷런에 구현되어 있는 [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)은 이번 텀프로젝트에서 구현한 경사 하강법(Gradient Descent) 기반의 방식과 어떤 차이가 있는지 자유롭게 서술하세요. [1점]\n\n    * 회귀 계수를 추정하는 과정에서의 다른 부분을 위주로 서술하시면 됩니다.","metadata":{"id":"vB_a8W-9qmNl"}}]}