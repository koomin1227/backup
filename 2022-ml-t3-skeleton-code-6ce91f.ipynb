{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 코드 시작 전 설명\n1. 음성 데이터에 대한 이론 및 과정을 잘 모르시는 분은 코드를 작성하기 전 overview를 한번 정독하시는 것을 권합니다.\n\n2. 스켈레톤 코드가 적혀있는 해당 ipynb 파일을 다운받아 자신의 작업공간(코랩, 캐글노트북 등등)에서 코드를 작성합니다.\n\n3. 각 코드에는 빈칸과 함께 구현 가이드 라인이 제공됩니다. 해당 가이드라인을 따라 코드를 작성하여 베이스 라인 성능을 달성합니다.\n   진행에 어려움이 있으신 경우에는 feature 가공에 사용되는 함수의 documentation 설명을 읽으시는 것을 권합니다.\n   \n4. (선택)베이스라인에 도달하였다면, 음성 feature를 새롭게 가공하여 추가적인 성능 향상을 달성해봅니다.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sklearn\nimport os\nfrom os.path import join","metadata":{"execution":{"iopub.status.busy":"2023-06-08T17:04:51.667801Z","iopub.execute_input":"2023-06-08T17:04:51.668663Z","iopub.status.idle":"2023-06-08T17:04:51.675778Z","shell.execute_reply.started":"2023-06-08T17:04:51.668614Z","shell.execute_reply":"2023-06-08T17:04:51.674521Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# DATA Load\n\nDATA_PATH = \"../input/2022-ml-project3\"\n# (참고) os.path.join 함수는 여러 문자열을 경로에 대한 문자열로 합쳐주는 함수입니다.\npd_train = pd.read_csv(join(DATA_PATH, 'train_data.csv'))\npd_test = pd.read_csv(join(DATA_PATH, 'test_data.csv'))\n\nprint(pd_train.info(), pd_test.info())","metadata":{"execution":{"iopub.status.busy":"2023-06-08T17:04:51.679917Z","iopub.execute_input":"2023-06-08T17:04:51.680535Z","iopub.status.idle":"2023-06-08T17:04:51.733022Z","shell.execute_reply.started":"2023-06-08T17:04:51.680480Z","shell.execute_reply":"2023-06-08T17:04:51.732062Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1008 entries, 0 to 1007\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   ID         1008 non-null   int64 \n 1   file_name  1008 non-null   object\n 2   emotion    1008 non-null   object\ndtypes: int64(1), object(2)\nmemory usage: 23.8+ KB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 432 entries, 0 to 431\nData columns (total 2 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   ID         432 non-null    int64 \n 1   file_name  432 non-null    object\ndtypes: int64(1), object(1)\nmemory usage: 6.9+ KB\nNone None\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Feature 추출**\n\n* extract_feature 함수는 음성 데이터를 읽어서 다음의 과정들을 거치며 feature를 추출합니다.(함수(입력) --> 출력)\n 1. Sampling&Quantization(continuos audio signal) --> Discrete audio signal\n 2. Short Time Fourier Transfrom(Discrete audio signal) --> Spectrogram\n 3. Mel-Filter(Spectrogram) --> Mel-Spectrogram\n 4. Discrete Cosine Transform(Mel-Spectrogram) --> Mel Frequency Cepstrum Coefficient\n \n위에 과정들은 모두 librosa 라이브러리에서 제공하는 함수들을 통하여 구현가능하며 사용되는 함수는 다음과 같습니다.\n\n1. librosa.load : Continuos audio signal(.wav파일)을 Discrete audio signal로 읽음. \n2. librosa.stft : Discrte audio signal을 windowing하여 프레임별로 나눈 후 FFT(Fast Fourier Transform)을 수행\n[stft_documentation](https://librosa.org/doc/latest/generated/librosa.stft.html?highlight=stft)\n3. librosa.feature.melspectrogram : stft 함수를 통하여 구한 spectrogram을 입력으로 Mel-filter를 적용함으로써 Mel-Spectrogram 생성[melspectrogram_documentation](https://librosa.org/doc/latest/generated/librosa.feature.melspectrogram.html?highlight=melspectrogram)\n4. librosa.feature.mfcc : melspectrogram 함수를 통하여 구한 Mel-spectrogram을 입력으로 DCT를 수행함으로써 MFCC를 생성 [mfcc_documentation](https://librosa.org/doc/latest/generated/librosa.feature.mfcc.html?highlight=mfcc)","metadata":{}},{"cell_type":"code","source":"import librosa\nimport glob, pickle\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport librosa, librosa.display \n\n\n# -------------------------------------\n# extract_feature(file_name): <= 코드를 추가하여 음성 feature를 추출하는 코드를 완성하세요\n# -------------------------------------\n# 목적: MFCC를 비롯한 여러 음성 feature를 추출\n# 입력인자: .wav 음성 파일의 이름\n# 출력인자: 입력 파일을 가공한 feature들 (Spectrogram, Mel-spectrogram, MFCC)\n# -------------------------------------\n\n\ndef extract_feature(file_name):\n    \n    \n    result=np.array([])\n    X, sample_rate = librosa.load(file_name, sr=22050)\n    specto=abs(librosa.stft(y=X,n_fft=512))\n   \n    \n    spectrogram_feature =np.mean(specto,axis=1)\n\n    power_specto=specto**2\n    mel=librosa.feature.melspectrogram(S=power_specto)\n    mel_db=librosa.amplitude_to_db(S=mel)\n    mel_spectrogram_feature=np.mean(mel_db,axis=1)\n   \n    mfcc=librosa.feature.mfcc(S=mel_db)\n    mfcc_feature =np.mean(mfcc,axis=1)\n    #-------------------------------------------------------------------------------\n\n    return spectrogram_feature, mel_spectrogram_feature, mfcc_feature ","metadata":{"execution":{"iopub.status.busy":"2023-06-08T17:04:51.734812Z","iopub.execute_input":"2023-06-08T17:04:51.735151Z","iopub.status.idle":"2023-06-08T17:04:51.747917Z","shell.execute_reply.started":"2023-06-08T17:04:51.735114Z","shell.execute_reply":"2023-06-08T17:04:51.746467Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"# **데이터 불러오기**\n\n### csv파일에 저장된 파일 이름과 학습용 label을 로드하여 학습 및 평가용 데이터를 불러오세요.","metadata":{}},{"cell_type":"code","source":"#DataFlair - Load the data and extract features for each sound file\n# (참고) tqdm 은 진행률에 대한 프로그레스바를 알려주는 라이브러리입니다. \n# for문에 자주 사용되며, 얼마나 진행되었는지를 시각적으로 확인할 수 있어 자주 사용됩니다.\nfrom tqdm.notebook import tqdm\ndef load_data(data_info, isTrain=True):\n    \n    PATH = join('/kaggle','input','2022-ml-project3')\n    if isTrain:\n        train_data = {'spectrogram':[],'mel':[],'mfcc':[]}#음성 feature들을 담는 dictionary\n        train_label = []#학습에 사용할 label을 담는 list\n        \n        file_list = data_info['file_name']\n        emotion_list = data_info['emotion']\n        # (참고) zip을 통해 2개 이상의 변수를 한 번에 넘길 수 있습니다.\n        # 궁금한 점이 있으면 zip() 내장함수를 검색해보시기 바랍니다\n        for file_name, emotion in tqdm(zip(file_list, emotion_list)):\n            train_path=os.path.join(PATH,'train_data','train_data',file_name)\n            \n            sp,mel,mfcc=extract_feature(train_path)\n            train_data['spectrogram'].append(sp)\n            train_data['mel'].append(mel)\n            train_data['mfcc'].append(mfcc)\n            \n            train_label.append(emotion)\n            \n            \n            #----------------------------------------------------------------------------------------- \n            \n        return train_data, np.array(train_label)\n    \n    else:\n        test_data = {'spectrogram':[],'mel':[],'mfcc':[]}#음성 feature들을 담는 dictionary\n        file_list = data_info['file_name']\n    \n        for file_name in tqdm(file_list):\n            test_path=os.path.join(PATH,'test_data','test_data',file_name)\n            \n            sp,mel,mfcc=extract_feature(test_path)\n            test_data['spectrogram'].append(sp)\n            test_data['mel'].append(mel)\n            test_data['mfcc'].append(mfcc)\n            \n            \n            \n            #----------------------------------------------------------------------------------------- \n            \n        return test_data\n\n#DataFlair - Split the dataset\ntrain_data, y_train = load_data(pd_train)\ntest_data = load_data(pd_test, isTrain=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T17:04:51.749606Z","iopub.execute_input":"2023-06-08T17:04:51.750127Z","iopub.status.idle":"2023-06-08T17:08:53.100334Z","shell.execute_reply.started":"2023-06-08T17:04:51.750085Z","shell.execute_reply":"2023-06-08T17:08:53.099067Z"},"trusted":true},"execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b9fd996032f42d5a3447958a7167b83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/432 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63bf06a9ecbb45e5aba2fc9623b9088d"}},"metadata":{}}]},{"cell_type":"markdown","source":"# 모델 학습 및 추론\n위에서 우리는 \n1. 음성 신호에 대하여 Short Time Fourier Transform(stft)를 통해 **spectrogram**을 구하고,\n2. 구한 spectrogram에 mel-filter를 씌워 **mel-spectrogram**을 구했으며,\n3. 마지막으로 mel-spectrogram에 Discrete cosine transform(DCT) 취해줌으로써 **mfcc**까지 계산하였습니다.\n\n위에서 구한 feature들은 모두 음성 데이터를 활용하는 다양한 작업(사람 음성 분류, 음성을 통한 감정 분류 등)에서 모델 학습에 사용할 수 있는 feature들입니다.\n각각의 **feature들에 따른 모델의 정확도가 얼마나 차이**나는지를 확인해봅시다.\n\n- 베이스라인의 분류기는 **RandomForestClassifier**를 사용합니다.\n- **random_state에 따른 성능 차이가 발생하오니 반드시 random_state를 1로 고정해주시길 바랍니다.**","metadata":{}},{"cell_type":"code","source":"y_train","metadata":{"execution":{"iopub.status.busy":"2023-06-08T17:08:53.103264Z","iopub.execute_input":"2023-06-08T17:08:53.108249Z","iopub.status.idle":"2023-06-08T17:08:53.123292Z","shell.execute_reply.started":"2023-06-08T17:08:53.108173Z","shell.execute_reply":"2023-06-08T17:08:53.121930Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"array(['disgust', 'surprised', 'disgust', ..., 'calm', 'fearful',\n       'neutral'], dtype='<U9')"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle=LabelEncoder()\ny_train=le.fit_transform(y_train)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T17:08:53.125574Z","iopub.execute_input":"2023-06-08T17:08:53.127220Z","iopub.status.idle":"2023-06-08T17:08:53.148359Z","shell.execute_reply.started":"2023-06-08T17:08:53.127157Z","shell.execute_reply":"2023-06-08T17:08:53.147083Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"execution":{"iopub.status.busy":"2023-06-08T17:08:53.149883Z","iopub.execute_input":"2023-06-08T17:08:53.150576Z","iopub.status.idle":"2023-06-08T17:08:53.161492Z","shell.execute_reply.started":"2023-06-08T17:08:53.150528Z","shell.execute_reply":"2023-06-08T17:08:53.160366Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"array([2, 7, 2, ..., 1, 3, 5])"},"metadata":{}}]},{"cell_type":"code","source":"#RandomForestClassifier로 음성 감정 분류 학습 및 평가\nfrom sklearn.ensemble import RandomForestClassifier\nsample = pd.read_csv(join(DATA_PATH,'sample_submit.csv'))\n\nfor feature_name in train_data.keys():\n    \n    rf=RandomForestClassifier(random_state=1)\n    X_train=[]\n    x_test=[]\n    \n    X_train=(train_data[feature_name])\n    x_test=((test_data[feature_name]))\n    \n    X_train=np.asarray(X_train)\n    x_test=np.asarray(x_test)\n    \n    \n    print(X_train.shape)\n    print(y_train.shape)\n    \n    rf.fit(X_train,y_train)\n    predict=rf.predict(x_test)\n    \n    \n    \n    \n    #Sample submit file 저장\n    sample['emotion'] = le.inverse_transform(predict)\n    sample.to_csv(join(feature_name+'.csv'),index=False,header=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-08T17:08:53.165281Z","iopub.execute_input":"2023-06-08T17:08:53.167205Z","iopub.status.idle":"2023-06-08T17:08:56.009388Z","shell.execute_reply.started":"2023-06-08T17:08:53.167144Z","shell.execute_reply":"2023-06-08T17:08:56.008387Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"(1008, 257)\n(1008,)\n(1008, 128)\n(1008,)\n(1008, 20)\n(1008,)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}