{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install konlpy  # 토큰화에 사용할 konlpy 라이브러리 설치","metadata":{"papermill":{"duration":13.875729,"end_time":"2023-05-08T06:10:55.254188","exception":false,"start_time":"2023-05-08T06:10:41.378459","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-23T04:37:49.392689Z","iopub.execute_input":"2023-05-23T04:37:49.393059Z","iopub.status.idle":"2023-05-23T04:38:03.257119Z","shell.execute_reply.started":"2023-05-23T04:37:49.393029Z","shell.execute_reply":"2023-05-23T04:38:03.255555Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting konlpy\n  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /opt/conda/lib/python3.10/site-packages (from konlpy) (4.9.2)\nCollecting JPype1>=0.7.0\n  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.6 in /opt/conda/lib/python3.10/site-packages (from konlpy) (1.23.5)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from JPype1>=0.7.0->konlpy) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.9)\nInstalling collected packages: JPype1, konlpy\nSuccessfully installed JPype1-1.4.1 konlpy-0.6.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os, random\nfrom tqdm.auto import tqdm # 진행도 시각화를 위한 라이브러리\n\nseed=42\nos.environ['PYTHONHASHSEED'] = str(seed)\nrandom.seed(seed)\nnp.random.seed(seed)","metadata":{"papermill":{"duration":0.022219,"end_time":"2023-05-08T06:10:55.287013","exception":false,"start_time":"2023-05-08T06:10:55.264794","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-23T04:38:03.263073Z","iopub.execute_input":"2023-05-23T04:38:03.263473Z","iopub.status.idle":"2023-05-23T04:38:03.367325Z","shell.execute_reply.started":"2023-05-23T04:38:03.263436Z","shell.execute_reply":"2023-05-23T04:38:03.366298Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# 데이터 불러오기","metadata":{"papermill":{"duration":0.009253,"end_time":"2023-05-08T06:10:55.306186","exception":false,"start_time":"2023-05-08T06:10:55.296933","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/2023-ml-project1/nsmc_train.csv\", index_col=0)\ntest_data = pd.read_csv(\"/kaggle/input/2023-ml-project1/nsmc_test.csv\", index_col=0)\nprint(train_data.shape)\nprint(test_data.shape)","metadata":{"papermill":{"duration":0.863577,"end_time":"2023-05-08T06:10:56.179319","exception":false,"start_time":"2023-05-08T06:10:55.315742","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-23T04:38:03.372276Z","iopub.execute_input":"2023-05-23T04:38:03.372629Z","iopub.status.idle":"2023-05-23T04:38:04.315889Z","shell.execute_reply.started":"2023-05-23T04:38:03.372599Z","shell.execute_reply":"2023-05-23T04:38:04.314648Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"(149993, 2)\n(49999, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"x_train = train_data[\"review\"]\ny_train = np.array(train_data[\"rating\"])","metadata":{"papermill":{"duration":0.032904,"end_time":"2023-05-08T06:10:56.282527","exception":false,"start_time":"2023-05-08T06:10:56.249623","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-23T04:38:04.317312Z","iopub.execute_input":"2023-05-23T04:38:04.317671Z","iopub.status.idle":"2023-05-23T04:38:04.325489Z","shell.execute_reply.started":"2023-05-23T04:38:04.317640Z","shell.execute_reply":"2023-05-23T04:38:04.323824Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df=pd.DataFrame(x_train)\ndf.columns\n# row 생략 없이 출력\npd.set_option('display.max_rows', None)\n# col 생략 없이 출력\npd.set_option('display.max_columns', None)\n#df.head(320)\nx_train.dtype","metadata":{"execution":{"iopub.status.busy":"2023-05-23T04:38:04.327754Z","iopub.execute_input":"2023-05-23T04:38:04.328107Z","iopub.status.idle":"2023-05-23T04:38:04.343450Z","shell.execute_reply.started":"2023-05-23T04:38:04.328078Z","shell.execute_reply":"2023-05-23T04:38:04.342185Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"dtype('O')"},"metadata":{}}]},{"cell_type":"markdown","source":"# 자연어 전처리\n## \\[Empty Module #1\\] 데이터 전처리\n### 데이터 전처리 수행\n> 먼저, 리뷰를 분류하는데 도움이 되거나, 머신러닝 처리에 어려운 단어들을 제거해봅시다.\n1. 아래 조건에 맞는 정규표현식을 작성하여 영어와 한글 문자를 제외한 특수문자나 이모지, 숫자 등을 제거해봅시다.\n  - <mark>한글 문자(초성 제외), 영어 대문자, 영어 소문자, 띄어쓰기 이외의 문자를 제외</mark>하는 정규표현식 작성\n2. 영어 단어의 경우 같은 단어들이 같은 토큰으로 분류될 수 있도록 <mark>대문자로 통일</mark>해줍니다.","metadata":{"papermill":{"duration":0.01003,"end_time":"2023-05-08T06:10:56.302834","exception":false,"start_time":"2023-05-08T06:10:56.292804","status":"completed"},"tags":[]}},{"cell_type":"code","source":"##########################################################################################\n# Empty Module #1\n# 입력: 자연어 상태의 리뷰 텍스트\n# 출력: 한글(초성 제외), 영어 대문자, 띄어쓰기로만 구성된 텍스트 \n# 입력 예시: \"안녕 Hello!!!:)ㅎㅎ반갑다.\"\n# 출력 예시: \"안녕 HELLO반갑다\"\n##########################################################################################\nimport re\n\npattern =re.compile('[^가-힣 A-Za-z\\s]+')\npattern2=re.compile('[a-z]')\n\n\ndef apply_regex(pattern, text):  # 정규표현식을 이용한 필터링 적용\n    text = re.sub(pattern, \"\", text)  # 정규표현식 패턴에 맞는 값들을 텍스트에서 제거\n    # 영어들을 찾아 대문자로 치환하는 코드 작성\n    text=re.sub(pattern2,lambda x: x.group(0).upper(),text)\n    #[a-z] 패턴은 소문자 x.group(0).upper()는 매칭된 문자열을 대문자로\n    return text\n\nx_train_preprocessed = [apply_regex(pattern, str(x[1])) for x in tqdm(x_train.iteritems(), total=len(x_train), desc=\"pre-processing data\")]","metadata":{"papermill":{"duration":0.982216,"end_time":"2023-05-08T06:10:57.296264","exception":false,"start_time":"2023-05-08T06:10:56.314048","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-23T04:38:04.345309Z","iopub.execute_input":"2023-05-23T04:38:04.345652Z","iopub.status.idle":"2023-05-23T04:38:05.312452Z","shell.execute_reply.started":"2023-05-23T04:38:04.345622Z","shell.execute_reply":"2023-05-23T04:38:05.311589Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/813706970.py:21: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  x_train_preprocessed = [apply_regex(pattern, str(x[1])) for x in tqdm(x_train.iteritems(), total=len(x_train), desc=\"pre-processing data\")]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pre-processing data:   0%|          | 0/149993 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07cfe9122dbd4a5abfeb03a1153c8173"}},"metadata":{}}]},{"cell_type":"code","source":"def apply_regex(pattern, text):  # 정규표현식을 이용한 필터링 적용\n    text = re.sub(pattern, \"\", text)  # 정규표현식 패턴에 맞는 값들을 텍스트에서 제거\n    # 영어들을 찾아 대문자로 치환하는 코드 작성\n    text=re.sub(pattern2,lambda x: x.group(0).upper(),text)\n    \n    return text\ntest_sen='안녕 Hello!!!:)ㅎㅎ반갑다.'\ntest_obj=np.array([test_sen]).astype('object')\n\ntest_proceed = [apply_regex(pattern, str(x[1])) for x in tqdm(np.ndenumerate(test_obj), total=len(test_obj), desc=\"pre-processing data\")]","metadata":{"execution":{"iopub.status.busy":"2023-05-23T04:38:05.313372Z","iopub.execute_input":"2023-05-23T04:38:05.313710Z","iopub.status.idle":"2023-05-23T04:38:05.344271Z","shell.execute_reply.started":"2023-05-23T04:38:05.313680Z","shell.execute_reply":"2023-05-23T04:38:05.343281Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"pre-processing data:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45a89293590a4ceaa4049a7d1398c71c"}},"metadata":{}}]},{"cell_type":"code","source":"print(test_proceed)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T04:38:05.345701Z","iopub.execute_input":"2023-05-23T04:38:05.346103Z","iopub.status.idle":"2023-05-23T04:38:05.351592Z","shell.execute_reply.started":"2023-05-23T04:38:05.346074Z","shell.execute_reply":"2023-05-23T04:38:05.350446Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"['안녕 HELLO반갑다']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## \\[Empty Module #2\\] 단어 토큰화\n### Open Korean Text(OKT)를 이용한 단어 토큰화(Tokenization)\n- 한국어 자연어 처리 라이브러리인 konlpy의 OKT 래퍼를 통하여 문장을 단어로 토큰화해봅시다.\n- 아래 도큐먼트를 참고하여 토큰화를 수행합니다.\n  - <mark>이때, OKT 클래스의 특정 매개변수를 이용해 어근화를 진행해줍니다. (도큐먼트 참고)</mark>\n- konlpy 도큐먼트: https://konlpy.org/ko/latest/api/konlpy.tag/#okt-class\n- OKT 도큐먼트: https://github.com/open-korean-text/open-korean-text","metadata":{"papermill":{"duration":0.014265,"end_time":"2023-05-08T06:10:57.323679","exception":false,"start_time":"2023-05-08T06:10:57.309414","status":"completed"},"tags":[]}},{"cell_type":"code","source":"##########################################################################################\n# Empty Module #2\n# 입력: 자연어 상태의 리뷰 데이터\n# 출력: 토큰화와 과정을 거쳐 단어들의 리스트로 변환된 데이터\n# 입력 예시: \"커피는 역시 학생회관 커피\"\n# 출력 예시: [\"커피\", \"는\", \"역시\", \"학생\", \"회관\", \"커피\"]\n##########################################################################################\nfrom konlpy.tag import Okt\nokt = Okt()\n\ndef tokenize_words(sentence):\n    sentence_tokenized =okt.morphs(sentence,stem=True)\n    return sentence_tokenized","metadata":{"papermill":{"duration":1.373034,"end_time":"2023-05-08T06:10:58.70747","exception":false,"start_time":"2023-05-08T06:10:57.334436","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-23T04:38:05.357906Z","iopub.execute_input":"2023-05-23T04:38:05.358686Z","iopub.status.idle":"2023-05-23T04:38:06.895304Z","shell.execute_reply.started":"2023-05-23T04:38:05.358640Z","shell.execute_reply":"2023-05-23T04:38:06.894106Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# 약 10-15분 정도 소요됩니다. \nx_train_tokenized = [tokenize_words(x) for x in tqdm(x_train_preprocessed, desc=\"tokenizing data\")]","metadata":{"papermill":{"duration":573.138395,"end_time":"2023-05-08T06:20:31.856663","exception":false,"start_time":"2023-05-08T06:10:58.718268","status":"completed"},"tags":[],"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-05-23T04:38:06.896480Z","iopub.execute_input":"2023-05-23T04:38:06.896890Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizing data:   0%|          | 0/149993 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e0a3e592034401689bee3ec2c588c93"}},"metadata":{}}]},{"cell_type":"code","source":"print(x_train_tokenized[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## \\[Empty Module #3\\] 불용어 제거\n- 조사를 비롯한 불용어들은 많은 횟수 등장하지만, 리뷰의 긍정과 부정 여부를 판단하는데는 도움이 되지 않습니다.\n- 데이터에서 아래 리스트로 정의된 불용어들을 제거해줍니다.","metadata":{"papermill":{"duration":0.226231,"end_time":"2023-05-08T06:20:32.313486","exception":false,"start_time":"2023-05-08T06:20:32.087255","status":"completed"},"tags":[]}},{"cell_type":"code","source":"stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']  #별다른 의미가 없는 불용어들\n\n\ndef exclude_stopwords(text):\n    # 위 리스트에 포함된 불용어들을 제거하는 코드 작성\n    result=[]\n    for w in text:\n        if w not in stopwords:\n            result.append(w)\n   \n    return result\n\n\nx_train_stopwords_excluded = [exclude_stopwords(x) for x in x_train_tokenized]","metadata":{"papermill":{"duration":1.425199,"end_time":"2023-05-08T06:20:33.964564","exception":false,"start_time":"2023-05-08T06:20:32.539365","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_train_stopwords_excluded[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## \\[Empty Module #4\\] 단어 임베딩\n### 단어 임베딩 코드 구현\n- 토큰화를 거쳐 분리된 단어들을 하나의 정수 값으로 매핑해주는 희소 표현법을 직접 구현해봅시다.\n- 입력된 단어가 새로운 단어라면 새로운 정수 값을 할당하고, 이전에 등장한 단어라면 이전에 할당한 정수를 할당하는 함수를 작성합니다.\n  - 단, <mark>테스트 데이터에 대해서는 새로운 단어가 등장하면 값을 할당하지 않습니다.</mark>","metadata":{"papermill":{"duration":0.227527,"end_time":"2023-05-08T06:20:34.416544","exception":false,"start_time":"2023-05-08T06:20:34.189017","status":"completed"},"tags":[]}},{"cell_type":"code","source":"##########################################################################################\n# Empty Module #4\n# 입력: 단어 토큰화된 데이터\n# 출력: 임베딩 과정을 거쳐, 각 단어가 하나의 실수 값으로 표현된 데이터\n# 입력 예시: [\"커피\", \"역시\", \"학생\", \"회관\", \"커피\"]\n# 출력 예시: [0, 1, 2, 3, 0]\n##########################################################################################\n\nembedding_dict = dict()  # 단어 임베딩을 위한 딕셔너리\nembedding_value = 0\n\ndef embed_tokens(sentence_tokenized, mode):\n    assert mode.upper() in [\"TRAIN\", \"TEST\"]\n    global embedding_value\n    \n    sentence_embedded = list()\n    for word in sentence_tokenized:\n      if mode.upper() == \"TRAIN\":\n        if word not in embedding_dict:\n          embedding_dict[word]=embedding_value\n          embedding_value+=1\n        sentence_embedded.append(embedding_dict[word])\n      elif mode.upper() == \"TEST\":\n        if word in embedding_dict:\n          sentence_embedded.append(embedding_dict[word])\n      \n    return sentence_embedded","metadata":{"papermill":{"duration":0.236545,"end_time":"2023-05-08T06:20:34.876645","exception":false,"start_time":"2023-05-08T06:20:34.6401","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 실행 시간이 제법 소요됩니다. 비정상이 아니니 걱정하지 않으셔도 됩니다.\nx_train_embedded = [embed_tokens(x, mode=\"TRAIN\") for x in tqdm(x_train_stopwords_excluded, desc=\"embedding data\")]\nprint(x_train_embedded[:5])\nprint(\"총 %d개의 단어가 임베딩되었습니다.\"%(embedding_value))","metadata":{"papermill":{"duration":1.328669,"end_time":"2023-05-08T06:20:36.428533","exception":false,"start_time":"2023-05-08T06:20:35.099864","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_train_stopwords_excluded[0])\nprint(x_train_embedded[0])\n\nprint(x_train_stopwords_excluded[1])\nprint(x_train_embedded[1])\n\nprint(x_train_stopwords_excluded[2])\nprint(x_train_embedded[2])\n\nprint(x_train_stopwords_excluded[3])\nprint(x_train_embedded[3])\n\nprint(x_train_stopwords_excluded[4])\nprint(x_train_embedded[4])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## \\[Empty Module #5\\] 문장 벡터화\n### Bag of Words 방법을 사용한 문장 벡터화\n- 캐글 프로젝트 설명 페이지의 Bag of Words 방법 설명을 참고하여 Bag of Words 방법을 직접 구현해봅시다.","metadata":{"papermill":{"duration":0.284019,"end_time":"2023-05-08T06:20:36.936606","exception":false,"start_time":"2023-05-08T06:20:36.652587","status":"completed"},"tags":[]}},{"cell_type":"code","source":"##########################################################################################\n# Empty Module #5\n# 입력: 임베딩 과정을 거친 데이터\n# 출력: BoW 형태로 변환되어, M차원의 고정된 크기를 가진 벡터로 변환된 데이터\n# 힌트: np.zeros((2, 3))는 [2, 3] 크기의 0으로 가득 찬 행렬을 생성합니다.\n##########################################################################################\nM = 45361\ndef to_BoW_representation(x):\n    shape = (len(x),M)# BoW는 어떤 shape를 가져야 할까요?\n    x_BoW = np.zeros(shape)\n    for i in tqdm(range(len(x)), desc=\"making BoW representation\"):\n        for word in x[i]:#현재문장의 단어마다\n            word_idx=(word)\n            x_BoW[i][word_idx]+=1#해당 단어 인덱스 위치 값 1 증가\n    return x_BoW\nx_train_BoW = to_BoW_representation(x_train_embedded)","metadata":{"papermill":{"duration":7.419859,"end_time":"2023-05-08T06:20:44.579436","exception":false,"start_time":"2023-05-08T06:20:37.159577","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nnp.set_printoptions(threshold=sys.maxsize)\n#x_train_BoW[1]\n#print(np.unique(x_train_BoW[0]))\nx_train_BoW.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## \\[Empty Module #6\\] 차원 축소\n- 우리가 만든 BoW는 수많은 리뷰에 등장하는 모든 단어들을 사용하여 만들어졌기 때문에, 엄청난 양의 단어들을 가지고 있습니다.\n- 그러나, 실제로 영화 리뷰에 쓰이는 단어들은 이보다 적기 때문에, 많은 단어들이 전체 데이터에서 실제로는 한번도 등장하지 않거나, 매우 조금 등장하면서 공간을 차지하고 있을 것 입니다.\n- 데이터의 크기를 줄여 머신러닝 모델이 중요한 정보에 집중할 수 있도록 해봅시다.\n- <mark>학습 데이터에서 50번 미만으로 등장한 단어들을 제외</mark>해줍니다.","metadata":{"papermill":{"duration":0.226259,"end_time":"2023-05-08T06:20:45.030446","exception":false,"start_time":"2023-05-08T06:20:44.804187","status":"completed"},"tags":[]}},{"cell_type":"code","source":"##########################################################################################\n# Empty Module #6\n# 입력: BoW 형태로 변환된 (N, M) 크기의 데이터\n# 출력: 등장 빈도가 적은 단어들을 제외한 (N, m) 크기의 더 작은 데이터\n##########################################################################################","metadata":{"papermill":{"duration":0.236099,"end_time":"2023-05-08T06:20:45.493465","exception":false,"start_time":"2023-05-08T06:20:45.257366","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 힌트\n# 1. 먼저 전체 데이터에서 각 단어가 등장한 횟수를 세어보세요.\n# 2. 그 다음, 등장 횟수가 50회 미만인 단어들을 찾습니다.\n# 3. 해당 단어들을 데이터에서 제거하는 코드를 작성합니다.\n# 4. 설계를 잘 하고 구현을 시작해야 어렵지 않습니다.","metadata":{"papermill":{"duration":7.501002,"end_time":"2023-05-08T06:20:53.222732","exception":false,"start_time":"2023-05-08T06:20:45.72173","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cnt=np.sum(x_train_BoW,axis=0)\n#print(np.unique(cnt))\n\nword_freq = np.sum(x_train_BoW,axis=0 )# 각 단어가 등장한 횟수를 세어줍니다.\n#print(word_freq[0:5])\nto_remove_idx = np.where(word_freq < 50) # 등장 횟수가 50회 미만인 단어들의 인덱스를 찾습니다.\n\n#print(to_remove_idx[0:5])\n\nx_train_BoW_reduced=np.delete(x_train_BoW,to_remove_idx,axis=1)\n#num_arr=np.asarray(to_remove)\n#print(num_arr.shape)\n#x_train_BoW_reduced = np.delete(x_train_BoW, to_remove, axis=1) # 해당 인덱스에 해당하는 열(단어)을 제거합니다.\n        \n    \n\nprint(\"원본 BoW 크기:\", x_train_BoW.shape)\nprint(\"차원 축소 후 크기:\", x_train_BoW_reduced.shape)","metadata":{"papermill":{"duration":12.114323,"end_time":"2023-05-08T06:21:05.627314","exception":false,"start_time":"2023-05-08T06:20:53.512991","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## \\[Empty Module #7\\]  분류 수행 및 제출: Bag of Words\n- 이제 모든 문장이 고정된 크기 $m$ 차원의 벡터로 변환되었습니다.\n- 원하는 모델을 사용하여, 각 문장의 영화에 대한 긍정적인 리뷰인지, 부정적인 리뷰인지 분류해봅시다.(Baseline 모델은 로지스틱 회귀입니다)\n- 그 다음, 결과를 기록하여 Kaggle에 제출해봅시다!","metadata":{"papermill":{"duration":0.22558,"end_time":"2023-05-08T06:21:06.079455","exception":false,"start_time":"2023-05-08T06:21:05.853875","status":"completed"},"tags":[]}},{"cell_type":"code","source":"##########################################################################################\n# Empty Module #7\n# 지금까지 전처리한 데이터로 분류를 수행하여, kaggle에 제출해봅시다.\n# Baseline은 로지스틱 회귀입니다.\n##########################################################################################\n# 분류기 정의 및 학습 수행 코드 작성\n\n","metadata":{"papermill":{"duration":43.097338,"end_time":"2023-05-08T06:21:49.406392","exception":false,"start_time":"2023-05-08T06:21:06.309054","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train,X_val,Y_train,Y_val=train_test_split(x_train_BoW_reduced,y_train,test_size=0.2,stratify=y_train,random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(Y_train.shape)\nprint(X_val.shape)\nprint(Y_val.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nC=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n\nfor i in range(0,10):\n    logi=LogisticRegression(max_iter=500,random_state=42,n_jobs=-1,C=C[i])\n    logi.fit(X_train,Y_train)\n    pred=logi.predict(X_val)\n    print(accuracy_score(Y_val,pred))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logi=LogisticRegression(max_iter=500,random_state=42,n_jobs=-1,C=0.6)\nlogi.fit(x_train_BoW_reduced,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TEST 데이터를 전처리\nx_test = test_data[\"review\"]\nx_test_preprocessed = [apply_regex(pattern, str(x[1])) for x in tqdm(x_test.iteritems(), total=len(x_test), desc=\"pre-processing data\")]\nx_test_tokenized = [tokenize_words(x) for x in tqdm(x_test_preprocessed, desc=\"tokenizing data\")]\nx_test_stopwords_excluded = [exclude_stopwords(x) for x in x_test_tokenized]\nx_test_embedded = [embed_tokens(x, mode=\"TEST\") for x in tqdm(x_test_stopwords_excluded, desc=\"embedding data\")]\n\nx_test_BoW = to_BoW_representation(x_test_embedded)\n\nword_freq = np.sum(x_train_BoW,axis=0 )# 각 단어가 등장한 횟수를 세어줍니다.\nto_remove_idx = np.where(word_freq < 50) # 등장 횟수가 50회 미만인 단어들의 인덱스를 찾습니다.\nx_test_BoW_reduced=np.delete(x_test_BoW,to_remove_idx,axis=1)","metadata":{"papermill":{"duration":275.661355,"end_time":"2023-05-08T06:26:28.190092","exception":false,"start_time":"2023-05-08T06:21:52.528737","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test_BoW_reduced.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TEST 데이터에 대한 예측 수행 코드 작성","metadata":{"papermill":{"duration":1.030924,"end_time":"2023-05-08T06:26:29.528144","exception":false,"start_time":"2023-05-08T06:26:28.49722","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = pd.read_csv(\"/kaggle/input/2023-ml-project1/sample_submission.csv\")\n# TEST 데이터에 대한 예측 값을 csv로 저장하는 코드 작성","metadata":{"papermill":{"duration":0.489598,"end_time":"2023-05-08T06:26:30.391147","exception":false,"start_time":"2023-05-08T06:26:29.901549","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = logi.predict(x_test_BoW_reduced)\nsubmit['rating']=pred\nsubmit.to_csv(\"submit_logi.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## \\[Empty Module #8\\] TF-IDF 적용\n- BoW에서는 고려하지 않는 각 단어들의 중요도를 고려하기 위해, TF-IDF를 적용해봅시다.\n- 캐글 프로젝트 설명 페이지의 설명을 참고하여 빈칸을 채워, TF-IDF를 구현해봅시다.","metadata":{"papermill":{"duration":0.301876,"end_time":"2023-05-08T06:26:30.999594","exception":false,"start_time":"2023-05-08T06:26:30.697718","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(embedding_dict['배우'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word=list(embedding_dict.keys())\nprint(len(word))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_train_BoW_reduced.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##########################################################################################\n# Empty Module #8\n# 빈칸을 적절히 채워넣어 TF-IDF를 위한 Inverse Document Frequency를 계산해봅시다.\n##########################################################################################\nfrom math import log\nN = len(x_train_BoW)  # 총 데이터 샘플의 수\n\ndef tf(t, d):\n    return d.count(t)\n\nresult=[]\n\ndef calculate_document_frequency(x):\n    result=(np.sum(x,axis=0))\n    return result \n        \n                \n#calculate_document_frequency(x_train_BoW_reduced)        \n\n    \nIDF=[]\ndef calculate_inverse_document_frequency(document_frequency):\n    for i in range(0,3127):\n        cnt=document_frequency[i]\n        print(cnt)\n        IDF.append(log(N/(cnt+1)))\n    return IDF\n\n   \n     \n    \ndocument_frequency=calculate_document_frequency(x_train_BoW_reduced)\n\ninverse_document_frequency =calculate_inverse_document_frequency(document_frequency)   ","metadata":{"papermill":{"duration":0.995899,"end_time":"2023-05-08T06:26:32.302887","exception":false,"start_time":"2023-05-08T06:26:31.306988","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_tfidf = x_train_BoW_reduced*inverse_document_frequency","metadata":{"papermill":{"duration":1.56492,"end_time":"2023-05-08T06:26:34.1783","exception":false,"start_time":"2023-05-08T06:26:32.61338","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## \\[Empty Module #9\\]  분류 수행 및 제출: TF-IDF\n- TF-IDF를 적용한 결과를 기록하여 Kaggle에 제출해봅시다!","metadata":{"papermill":{"duration":0.302103,"end_time":"2023-05-08T06:26:34.784772","exception":false,"start_time":"2023-05-08T06:26:34.482669","status":"completed"},"tags":[]}},{"cell_type":"code","source":"##########################################################################################\n# Empty Module #9\n# TEST 데이터에 TF-IDF를 적용하여 모델을 학습, 예측을 수행하고 kaggle에 제출해봅시다.\n# 이때, BoW와 같은 모델을 사용하여 성능을 비교해봅시다.\n##########################################################################################\n# 분류기 정의 및 학습 수행 코드 작성\n\nlogi=LogisticRegression(max_iter=500,random_state=42,n_jobs=-1,C=0.6)\nlogi.fit(x_train_tfidf,y_train)","metadata":{"papermill":{"duration":50.621027,"end_time":"2023-05-08T06:27:25.711221","exception":false,"start_time":"2023-05-08T06:26:35.090194","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TEST 데이터를 전처리하는 코드 작성\nx_test_tfidf = x_test_BoW_reduced*inverse_document_frequency","metadata":{"papermill":{"duration":1.234079,"end_time":"2023-05-08T06:27:28.588531","exception":false,"start_time":"2023-05-08T06:27:27.354452","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 예측을 수행하는 코드 작성\npred = logi.predict(x_test_tfidf)","metadata":{"papermill":{"duration":0.569811,"end_time":"2023-05-08T06:27:29.46472","exception":false,"start_time":"2023-05-08T06:27:28.894909","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = pd.read_csv(\"/kaggle/input/2023-ml-project1/sample_submission.csv\")\n# TEST 데이터에 대한 예측 값을 csv로 저장하는 코드 작성\nsubmit['rating']=pred\nsubmit.to_csv(\"submit_logi_tfidf.csv\", index=False)","metadata":{"papermill":{"duration":0.401319,"end_time":"2023-05-08T06:27:30.302375","exception":false,"start_time":"2023-05-08T06:27:29.901056","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 결과 비교 TIP\n- BoW와 TF-IDF에 같은 모델을 적용하여 성능의 차이를 비교해봅시다.\n- 각각의 방법론에 여러가지 모델을 적용하며, 사용한 방법에 따라 더 적절한 모델이 있는지 고려해봅시다.\n- 실험 결과들을 토대로, 왜 이런 결과가 도출되었는지 고민해봅시다.","metadata":{}}]}